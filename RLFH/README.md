## General
This example demonstrates finetuning language models through reinforcement learning from human feedback for language alignment.

## Dataset
The original dataset has around 40K movie reviews with positive and negative reviews labeled as 1 and 0, respectively. In this example, 5K positive reviews and 5K negative reviews are sampled for finetuning the model. The dataset is splited into three parts, 80% for training, 10% for validation and 10% for testing. 

Dataset Link: https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis

## Model
Finetuning language models via RLFH (Reinforcement Learning from Human Feedback) mainly consists of three steps, including supervised finetuning, reward model training and reinforcement learning from human feedback finetuning.

Step 1. supervised finetuning (SFT Model) <br/>

This step focus on finetuning a pretrained model for language modeling on the dataset we are interested in. The language model here is GPT2 (Generative Pre-Training) Small model with a language modeling head (GPT2LMHeadModel) from Hugging Face. The model consists of the embedding layer, 12 decoder layers. The weights of the 10th and 11th decoder layers (index starting from 0) are finetuned for 10 epoches, with all other model parameters frozen.

> Model input: {Prompt}; Model output: {Response}

Here, both {Prompt} and {Response} indicate a sequence of words. The prompt occupies around 10% of the tokens for each prompt response pair.

Step 2: reward model training (Reward Model) <br/>

This step focus on traning a reward model for generating a scalar value for each prompt response pair, which will be further used in the step 3. The reward model here is BERT (Bidirectional Encoder Representations from Transformers) Base model with a sequence classification head (BertForSequenceClassification) from Hugging Face. The model consists of the embedding layer, 12 encoder layers, and classification layer. The weights of the 11th encoder layer (index start from 0) and the classification layer are finetuned for 10 epoches, with all other model parameters frozen.

> Model input: {Prompt} + {Response}; Model output: reward

Here, input represents the combination of the prompt and response, and the output is the logit for postive class presenting scalar reward.

Step 3: reinforcement learning from human feedback finetuning (RL Model) <br/>

This step focus on further finetuning the model from step 1 (SFT model) according to the rewards generated by the reward model from step 2. The PPO algorithm is adopted for the purpose of finetuning. For each prompt, one response is generated from the RL model. Moreover, to compare with the SFT model, the probability of that response conditioned on the prompt is also calculated for the SFT model. The objective function includes the rewards of the prompt response pair from RL model and the KL divergence for response probabilities from both models. The KL divergence is a penalty term to prevent rapid updates of the model parameters. 

## Evaluation

### Supervised finetuning

<img src="figures/sft_loss.png" width="500" />

**Figure 1. SFT model loss on the train and validation dataset during training.**

| | Loss | Perplexity |
| --- | --- | --- |
| Train | 3.233 | 25.358 |
| Validation | 3.536 | 34.343 |
| Test | 3.507 | 33.362 |

**Table 1. Loss and perplexity on train/validation/test dataset.**

Figure 1 shows the train and validation loss at different epoches during training for the SFT model. After supervised finetuning, the model achieves a perplexity of 33.362 on the test dataset as shown in Table 1.

### Reward model training

<img src="figures/rm_loss.png" width="500" />

**Figure 2. Reward model loss on the train and validation dataset during training.**

| | Accuracy | Precison | Recall | F1 | 
| --- | --- | --- | --- | --- |
| Train | 0.941 | 0.941 | 0.941 | 0.941 |
| Validation | 0.899 | 0.899 | 0.899 | 0.899 |
| Test | 0.912 | 0.912 | 0.912 | 0.912 |

**Table 2. Summary of various metrics on train/validation/test dataset.**

| | 0 | 1 |
| --- | --- | --- |
| 0 | 429 | 41 |
| 1 | 47 | 483 |

**Table 3. Confusion matrix on test dataset. 1 and 0 represents the positive and negative reviews, respectively.**

Figure 2 shows the train and validation loss at different epoches during training for the reward model. After finetuning, the model achieves an accuracy of 91.2% on the test dataset as shown in Table 2. Table 3 illustrates the confusion matrix on the test dataset with 1/0 indicating positive/negative class, respectively. The model exhibits similar performance on detecting positive and negative class. 

### Reinforcement learning from human feedback
 
<img src="figures/train_reward.png" width="500" />

**Figure 3. Average rewards on the train dataset during training.**

| SFT Model | RL Model |
|---|---|
| <img src="figures/test_reward_before.png" /> | <img src="figures/test_reward_after.png" /> |

**Figure 4. Rewards on the test dataset from SFT model (left) and RL model (right).**

<img src="figures/map.png" width="500"/>

**Figure 5. Reward map for SFT and RL model.**

Figure 3 shows the increasing average rewards during training, indicating prompt reponse pairs gradaully become more positive. Figure 4 shows the distribution of rewards for each question response pair, generated by the SFT model and RL model, from the test dataset. For the SFT model, the reward distribution is symmetric and flat indicating the SFT model does not have any bias to generate postive or negative response. Nonetheless, for the RL model, the reward distribution mainly concentrates on the right end, indicating a tendency to generate positive responses given the same prompts. Figure 5 further shows the 2d reward distribution formed by rewards of SFT and RL model. For most of cases, no matter SFT model generates positive or negative responses, the RL model tends to increases the postiveness of the responses. Interestingly, there are also few cases where the SFT model generates positive responses while the RL model generates negative responses.

## Reference
1. https://huggingface.co/docs/transformers/en/model_doc/bert
2. https://huggingface.co/docs/transformers/en/model_doc/gpt2
3. https://huggingface.co/docs/trl/index
4. Kenton, Jacob Devlin Ming-Wei Chang, and Lee Kristina Toutanova. "Bert: Pre-training of deep bidirectional transformers for language understanding." Proceedings of naacL-HLT. Vol. 1. 2019.
5. Radford, Alec, et al. "Language models are unsupervised multitask learners." OpenAI blog 1.8 (2019): 9.
6. Ouyang, Long, et al. "Training language models to follow instructions with human feedback." Advances in neural information processing systems 35 (2022): 27730-27744.
